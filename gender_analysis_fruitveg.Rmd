---
title: "gender_analysis_fruitveg"
output: html_document
date: '2022-10-12'
---

This is an analysis based on the paper "The association of food parenting practices with adolescentsâ€™ dietary
behaviors differ by youth gender, but not by parent gender" by Deslippe et al., 2022

```{r message=FALSE, warning=FALSE, results='hide'}
library(vroom)
library(plyr)
library(ggplot2)
library(psych)
library(tidyverse)
library(OpenMx)
library(dplyr)
library(lavaan)
library(semPlot)
library(knitr)
library(kableExtra)
library(GGally)
library(lavaanPlot)
```

```{r message=FALSE, warning=FALSE, results='hide'}
df<-vroom("/Users/gracer/Library/CloudStorage/OneDrive-SharedLibraries-UniversityofWyoming/M2AENAD Lab - Documents/RESEARCH/multi-state/data/final_df.csv")
names(df)
```
## Check normality of Fast Food
```{r}
str(df)
hist(df$FV_freq) # not normal
min(df$FV_freq)
C = 1 #needed to correct for 0s
hist(log10(df$FV_freq+C))# good
df$FV_log10<-log10(df$FV_freq+C)
df$FV_log10_scale<-scale(df$FV_log10)
hist(df$FV_log10_scale)# good
```

# Fast Food Models
## Basic model
food security, parent education, child sex, race/ethnicity, independent eating occasions (from covar analysis)
```{r}
m1 <-'FV_log10_scale ~ IND_scale + EXP_scale + AVA_log10_scale + MON_scale + AUT_sqrt_scale + MOD_log10_scale + parent_gender  + RACE_ETH + child_SEX + parent_ED + FOOD_SEC_CAT + iEO_log10'
```

## Confirmatory factor analysis 
```{r}
fit1 <- cfa(m1, data = df)
```

## Summary of SSB CFA
```{r}
summary(fit1, standardized=T,rsquare=T)
```
# Summary of CFA
IND_scale, MON_scale, MOD_log10_scal, parent_ED, FOOD_SEC_CAT, iEO_log10 
## Visualization of Junk food CFA
```{r}
semPaths(fit1, 'std', layout = 'spring')
```
```{r}
df$parent_gender<-as.ordered(df$parent_gender)
```

## Interaction models
IND_scale, AVA_log10_scal, MON_scale
```{r}
med_MOD_ <- "
         # mediator, a path
         parent_gender ~ a*MOD_log10_scale+child_SEX+FOOD_SEC_CAT+iEO_log10

         # b path
         FV_log10_scale ~ b*parent_gender

         # direct effect 
         FV_log10_scale ~ cp * MOD_log10_scale+child_SEX+FOOD_SEC_CAT+iEO_log10

         # indirect 
         ab := a*b
         
         # Total effects
         total := cp + (a*b)
         "
med_MON_ <- "
         # mediator, a path
         parent_gender ~ a*MON_scale+child_SEX+FOOD_SEC_CAT+iEO_log10

         # b path
         FV_log10_scale ~ b*parent_gender

         # direct effect 
         FV_log10_scale ~ cp * MON_scale+child_SEX+FOOD_SEC_CAT+iEO_log10

         # indirect 
         ab := a*b
         
         # Total effects
         total := cp + (a*b)
         "
med_IND_ <- "
         # mediator, a path
         parent_gender ~ a*IND_scale+child_SEX+FOOD_SEC_CAT+iEO_log10

         # b path
         FV_log10_scale ~ b*parent_gender

         # direct effect 
         FV_log10_scale ~ cp * IND_scale+child_SEX+FOOD_SEC_CAT+iEO_log10

         # indirect 
         ab := a*b
         
         # Total effects
         total := cp + (a*b)
         "
```



```{r}
set.seed(1234)
```

### Running the models
```{r}
IND_sem2 <- sem(med_IND_2, data = df, se = "bootstrap", bootstrap = 5000, estimator = "DWLS")

IND_sem <- sem(med_IND_, data = df, se = "bootstrap", bootstrap = 5000, estimator = "DWLS")
MON_sem <- sem(med_MON_, data = df, se = "bootstrap", bootstrap = 5000, estimator = "DWLS")
MOD_sem <- sem(med_MOD_, data = df, se = "bootstrap", bootstrap = 5000, estimator = "DWLS")
```


### Parameter estimates 
```{r}
summary(IND_sem, standardized = TRUE) 
summary(MON_sem, standardized = TRUE) 
summary(MOD_sem, standardized = TRUE) 
```
```{r}
makincopies<-function(X, Y){
  sumsum<-describeBy(X, Y)
  test<-map2_dfr(as.data.frame(sumsum[1]), as.data.frame(sumsum[2]), ~ tibble(A = .x, B = .y))
  nms<-c('var','n', 'mean','sd','median','trimmed','mad','min','max','range','skew','kurtosis','se')
  df_transpose = as.data.frame(t(test))
  colnames(df_transpose)<-nms
  df_transpose<-rownames_to_column(df_transpose)
  return(df_transpose)
}
cbPalette <- c( "#6D8DF3","#CC4F7E")
summarized_df<-makincopies(df$IND_scale, df$parent_gender)
summarized_df2<-makincopies(df$FV_log10, df$parent_gender)
```

A = male
B = Female
```{r}
ggplot(summarized_df, aes(x=rowname, y=mean, fill = rowname)) + 
    geom_bar(position=position_dodge(), stat="identity") +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd),
                  width=.2,                    # Width of the error bars
                  position=position_dodge(.9))+ labs(title = "Indulgence by parent gender", x = "Parent gender", y = "Indulgence (scaled)") + scale_fill_manual(values=cbPalette) + scale_x_discrete(labels=c("A" = "Male", "B" = "Female")) + theme_classic()+ theme(legend.position = "none")
```
```{r}
ggplot(summarized_df2, aes(x=rowname, y=mean, fill = rowname)) + 
    geom_bar(position=position_dodge(), stat="identity") +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd),
                  width=.2,                    # Width of the error bars
                  position=position_dodge(.9))+ labs(title = "Fruit and vegetable intake by parent gender", x = "Parent gender", y = "Fruit and vegetable intake (log10 transformed and scaled)") + scale_fill_manual(values=cbPalette) + scale_x_discrete(labels=c("A" = "Male", "B" = "Female")) + theme_classic()+ theme(legend.position = "none")
```
0 = male
1 = female
```{r}
cbPalette <- c( "#6D8DF3","#CC4F7E")

ggplot(df, aes(x=IND_scale, y=FV_log10_scale, color=parent_gender)) +geom_smooth(method=lm)  +
    geom_point(shape=1) + theme_classic() + labs(title = "Interaction of parent gender on indulgence and fruit and vegetable intake", x = "Indulgence (scaled)", y = "Fruit and vegetable intake (log10 transformed and scaled)", color = "Parent gender \n0 is male, 1 is female") + scale_colour_manual(values=cbPalette)

```

```{r}
labels <- list(child_SEX = "child sex", IND_scale = "Indulgence", iEO_log10 = "Independent eating occasions", parent_gender = "Parent gender", FOOD_SEC_CAT = "Food security", FV_log10_scale = "Fruit and veggie intake")

lavaanPlot(model = IND_sem,labels = labels, node_options = list(shape = "box", fontname = "Helvetica"), edge_options = list(color = "grey"), coefs = T, sig = .1, stand = TRUE)
```



# Summary of results
* Indulgence related (0.06)
* Indulgence trending
## Model with grouping
```{r}
IND_group <- ' 
             # mediator, a path
             parent_gender ~ c(ag1,ag2)*IND_scale+FOOD_SEC_CAT+iEO_log10
             FV_log10_scale ~ c(bg1,bg2)*parent_gender 
           
             # direct effect
             FV_log10_scale ~  c(cg1, cg2)*IND_scale+FOOD_SEC_CAT+iEO_log10

           # indirect effect (a*b)
             abg1 := ag1*bg1 #group 1
             abg2 := ag2*bg2 #group 2

           # total effect
             totalg1 := cg1 + (ag1*bg1)
             totalg2 := cg2 + (ag2*bg2)
         '
```


## Running group models
```{r}
fit_group_IND <- sem(IND_group, data = df, group="child_SEX", 
                 se="boot", bootstrap=5000, meanstructure = TRUE, estimator = "DWLS")
```

```{r}
fit_group_IND <- sem(IND_group, data = df, group="child_SEX", 
                 se="boot", bootstrap=5000, meanstructure = TRUE, estimator = "DWLS")
```

```{r}
summary(fit_group_IND, fit.measures=T, standardized=T, ci=TRUE)
```

```{r}
all.constraints<- 'ag1 == ag2
                  bg1 == bg2
                  cg1 == cg2'
```


```{r}
lavTestWald(fit_group_IND, #the name of the Lavaan 'fitted' object
            constraints = all.constraints) #the name of our previously specified paths that we would like to test
```
* Child gender is not related



```{r}
## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    library(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}
```

```{r}
normDataWithin <- function(data=NULL, idvar, measurevar, betweenvars=NULL,
                           na.rm=FALSE, .drop=TRUE) {
    library(plyr)

    # Measure var on left, idvar + between vars on right of formula.
    data.subjMean <- ddply(data, c(idvar, betweenvars), .drop=.drop,
     .fun = function(xx, col, na.rm) {
        c(subjMean = mean(xx[,col], na.rm=na.rm))
      },
      measurevar,
      na.rm
    )

    # Put the subject means with original data
    data <- merge(data, data.subjMean)

    # Get the normalized data in a new column
    measureNormedVar <- paste(measurevar, "_norm", sep="")
    data[,measureNormedVar] <- data[,measurevar] - data[,"subjMean"] +
                               mean(data[,measurevar], na.rm=na.rm)

    # Remove this subject mean column
    data$subjMean <- NULL

    return(data)
}
```

```{r}
## Summarizes data, handling within-subjects variables by removing inter-subject variability.
## It will still work if there are no within-S variables.
## Gives count, un-normed mean, normed mean (with same between-group mean),
##   standard deviation, standard error of the mean, and confidence interval.
## If there are within-subject variables, calculate adjusted values using method from Morey (2008).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   betweenvars: a vector containing names of columns that are between-subjects variables
##   withinvars: a vector containing names of columns that are within-subjects variables
##   idvar: the name of a column that identifies each subject (or matched subjects)
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySEwithin <- function(data=NULL, measurevar, betweenvars=NULL, withinvars=NULL,
                            idvar=NULL, na.rm=FALSE, conf.interval=.95, .drop=TRUE) {

  # Ensure that the betweenvars and withinvars are factors
  factorvars <- vapply(data[, c(betweenvars, withinvars), drop=FALSE],
    FUN=is.factor, FUN.VALUE=logical(1))

  if (!all(factorvars)) {
    nonfactorvars <- names(factorvars)[!factorvars]
    message("Automatically converting the following non-factors to factors: ",
            paste(nonfactorvars, collapse = ", "))
    data[nonfactorvars] <- lapply(data[nonfactorvars], factor)
  }

  # Get the means from the un-normed data
  datac <- summarySE(data, measurevar, groupvars=c(betweenvars, withinvars),
                     na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)

  # Drop all the unused columns (these will be calculated with normed data)
  datac$sd <- NULL
  datac$se <- NULL
  datac$ci <- NULL

  # Norm each subject's data
  ndata <- normDataWithin(data, idvar, measurevar, betweenvars, na.rm, .drop=.drop)

  # This is the name of the new column
  measurevar_n <- paste(measurevar, "_norm", sep="")

  # Collapse the normed data - now we can treat between and within vars the same
  ndatac <- summarySE(ndata, measurevar_n, groupvars=c(betweenvars, withinvars),
                      na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)

  # Apply correction from Morey (2008) to the standard error and confidence interval
  #  Get the product of the number of conditions of within-S variables
  nWithinGroups    <- prod(vapply(ndatac[,withinvars, drop=FALSE], FUN=nlevels,
                           FUN.VALUE=numeric(1)))
  correctionFactor <- sqrt( nWithinGroups / (nWithinGroups-1) )

  # Apply the correction factor
  ndatac$sd <- ndatac$sd * correctionFactor
  ndatac$se <- ndatac$se * correctionFactor
  ndatac$ci <- ndatac$ci * correctionFactor

  # Combine the un-normed means with the normed results
  merge(datac, ndatac)
}
```


```{r}
#save.image(file = "/Users/gracer/Library/CloudStorage/OneDrive-SharedLibraries-UniversityofWyoming/M2AENAD Lab - Documents/RESEARCH/multi-state/data/FVanalysis.RData")
```

```{r}
load(file = "/Users/gracer/Library/CloudStorage/OneDrive-SharedLibraries-UniversityofWyoming/M2AENAD Lab - Documents/RESEARCH/multi-state/data/FVanalysis.RData")
```